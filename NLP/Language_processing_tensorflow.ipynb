{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import essential libraries for Processing\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create our list of word or lets say day in python list format\n",
    "phrase = [\n",
    "    'I love my dog',\n",
    "    'I love my cat',\n",
    "    'sita love gita',\n",
    "    'hari is son of shyam',\n",
    "    'but shyam doesnot like gita'\n",
    "    'one when geeta was old she has gone out from home',\n",
    "    'his email is fogalop@gmail.com'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer with maximum first 100 most used words\n",
    "# it means if there are thousands of books and words only the 100 \n",
    "# most used words will be used\n",
    "tokenizer = Tokenizer(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the tokenizer\n",
    "tokenizer.fit_on_texts(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now get the word_index parameter from tokenizer \n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'i': 2, 'my': 3, 'is': 4, 'shyam': 5, 'dog': 6, 'cat': 7, 'sita': 8, 'gita': 9, 'hari': 10, 'son': 11, 'of': 12, 'but': 13, 'doesnot': 14, 'like': 15, 'gitaone': 16, 'when': 17, 'geeta': 18, 'was': 19, 'old': 20, 'she': 21, 'has': 22, 'gone': 23, 'out': 24, 'from': 25, 'home': 26, 'his': 27, 'email': 28, 'fogalop': 29, 'gmail': 30, 'com': 31}\n"
     ]
    }
   ],
   "source": [
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text to sequences model creates the list of number associated with words\n",
    "sequences = tokenizer.texts_to_sequences(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 1, 3, 6], [2, 1, 3, 7], [8, 1, 9], [10, 4, 11, 12, 5], [13, 5, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26], [27, 28, 4, 29, 30, 31]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create new data set with new words\n",
    "test_data = [\n",
    "    'joy love to go out',\n",
    "    'suntali is beautiful girl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 24], [4]]\n"
     ]
    }
   ],
   "source": [
    "# fit the new model in test model\n",
    "test_seq = (tokenizer.texts_to_sequences(test_data))\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new word are not shown form test modle.\n",
    "# so the data set should be proper in before training the model\n",
    "#####################################\n",
    "# we have little trick here to rigd out of this problem\n",
    "# that is while defining the model include \"<OOV>\"\n",
    "# this will use only the word which are required and not use the word which are not required\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pad_sequences\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100, oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordindex = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': 1,\n",
       " 'i': 2,\n",
       " 'my': 3,\n",
       " 'is': 4,\n",
       " 'shyam': 5,\n",
       " 'dog': 6,\n",
       " 'cat': 7,\n",
       " 'sita': 8,\n",
       " 'gita': 9,\n",
       " 'hari': 10,\n",
       " 'son': 11,\n",
       " 'of': 12,\n",
       " 'but': 13,\n",
       " 'doesnot': 14,\n",
       " 'like': 15,\n",
       " 'gitaone': 16,\n",
       " 'when': 17,\n",
       " 'geeta': 18,\n",
       " 'was': 19,\n",
       " 'old': 20,\n",
       " 'she': 21,\n",
       " 'has': 22,\n",
       " 'gone': 23,\n",
       " 'out': 24,\n",
       " 'from': 25,\n",
       " 'home': 26,\n",
       " 'his': 27,\n",
       " 'email': 28,\n",
       " 'fogalop': 29,\n",
       " 'gmail': 30,\n",
       " 'com': 31}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(phrase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = pad_sequences(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3, 2, 4, 7], [3, 2, 4, 8], [9, 2, 10], [11, 5, 12, 13, 6], [14, 6, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27], [28, 29, 5, 30, 31, 32]]\n",
      "\n",
      " [[ 0  0  0  0  0  0  0  0  0  0  0  3  2  4  7]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  3  2  4  8]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  9  2 10]\n",
      " [ 0  0  0  0  0  0  0  0  0  0 11  5 12 13  6]\n",
      " [14  6 15 16 17 18 19 20 21 22 23 24 25 26 27]\n",
      " [ 0  0  0  0  0  0  0  0  0 28 29  5 30 31 32]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('\\n',padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here in paded and sequences, the sequences data is similar with paded\n",
    "# but paded has put 0 infront of sequences data\n",
    "# its because the pading sequence is managing length of data as per \n",
    "# the longest scentence or data available in list\n",
    "##########################################\n",
    "# If we want to fixed the paded length we can also do this so that the \n",
    "# model put limit on it.\n",
    "# but what if we limit the length and the word is longer in data set?\n",
    "# in that case we can truncating either choping the word from last with post or from the begning with pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "paded = pad_sequences(sequences, padding='pre',\n",
    "                     truncating='post', maxlen=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3  2  4  7]\n",
      " [ 0  3  2  4  8]\n",
      " [ 0  0  9  2 10]\n",
      " [11  5 12 13  6]\n",
      " [14  6 15 16 17]\n",
      " [28 29  5 30 31]]\n"
     ]
    }
   ],
   "source": [
    "print(paded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
